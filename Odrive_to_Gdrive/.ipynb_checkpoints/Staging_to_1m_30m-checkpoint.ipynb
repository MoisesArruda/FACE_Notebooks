{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5871d5c9",
   "metadata": {},
   "source": [
    "### Este notebook possui as seguintes finalidades\n",
    "\n",
    "1 - Consultar os arquivos da pasta Staging no Google Drive.\n",
    "2 - Pegar todos os arquivos e transformar em DF.\n",
    "3 - Retornar os cabeçalhos.\n",
    "4 - Gerar CSV final\n",
    "5 - Armazenar os arquivos .dat nas suas pastas corretas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43df890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Acessar sistema operacional\n",
    "import os\n",
    "# Irá retornar uma lista de arquivos\n",
    "import glob\n",
    "# Manipulação de arquivos e diretórios\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e1ba059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onde esses arquivos serão armazenados\n",
    "pasta_Staging = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging'\n",
    "# Arquivos de 1 minuto\n",
    "pasta_Gdrive_1m = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_Meteo_1m_2023\\Ascii'\n",
    "# Arquivos de 30 minutos\n",
    "pasta_Gdrive_30m = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_meteo_Media_30m_2023\\Ascii'\n",
    "# Caminho csv\n",
    "arquivo_csv = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_Meteo_1m_2023.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1121dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Arquivos de 1 Minuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9796bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter a lista de arquivos .dat na pasta 1 minuto\n",
    "arquivos_staging = glob.glob(f'{pasta_Staging}\\\\*.dat')\n",
    "\n",
    "# Se não houve nenhum arquivo\n",
    "if not arquivos_staging:\n",
    "    print('Erro: A pasta está vazia.')\n",
    "    exit() # Terminar o programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e40a8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BlueShift\\AppData\\Local\\Temp\\ipykernel_56436\\48561845.py:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(arquivo_csv,error_bad_lines=False)\n",
      "Skipping line 18935: expected 1 fields, saw 2\n",
      "Skipping line 35030: expected 1 fields, saw 2\n",
      "Skipping line 59221: expected 1 fields, saw 2\n",
      "Skipping line 63759: expected 1 fields, saw 2\n",
      "Skipping line 115576: expected 1 fields, saw 2\n",
      "Skipping line 142942: expected 1 fields, saw 2\n",
      "Skipping line 145721: expected 1 fields, saw 2\n",
      "Skipping line 146763: expected 1 fields, saw 2\n",
      "Skipping line 147231: expected 1 fields, saw 2\n",
      "Skipping line 148295: expected 1 fields, saw 2\n",
      "Skipping line 149697: expected 1 fields, saw 2\n",
      "Skipping line 155238: expected 1 fields, saw 2\n",
      "Skipping line 155609: expected 1 fields, saw 2\n",
      "Skipping line 155728: expected 1 fields, saw 2\n",
      "Skipping line 155821: expected 1 fields, saw 2\n",
      "Skipping line 157332: expected 1 fields, saw 2\n",
      "Skipping line 174659: expected 1 fields, saw 2\n",
      "Skipping line 178573: expected 1 fields, saw 2\n",
      "Skipping line 178636: expected 1 fields, saw 2\n",
      "Skipping line 178740: expected 1 fields, saw 2\n",
      "Skipping line 180720: expected 1 fields, saw 2\n",
      "Skipping line 197645: expected 1 fields, saw 2\n",
      "Skipping line 302385: expected 1 fields, saw 2\n",
      "Skipping line 331982: expected 1 fields, saw 2\n",
      "Skipping line 332571: expected 1 fields, saw 2\n",
      "Skipping line 333430: expected 1 fields, saw 2\n",
      "Skipping line 340585: expected 1 fields, saw 2\n",
      "Skipping line 340849: expected 1 fields, saw 2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índices das linhas problemáticas: []\n"
     ]
    }
   ],
   "source": [
    "# Há algumas linhas com erro no CSV, irei pula-las e relatar \n",
    "linhas_problematicas = []\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(arquivo_csv,error_bad_lines=False)\n",
    "except pd.errors.ParserErros as e:\n",
    "    linhas_com_erros = [int(line.split()[0]) for line in str(e).split('\\n')[1:-1]]\n",
    "    \n",
    "    # Adicione os índices das linhas problemáticas à lista linhas_problematicas\n",
    "    linhas_problematicas.extend(linhas_com_erros)\n",
    "    \n",
    "print(\"Índices das linhas problemáticas:\", linhas_problematicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fad979cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da função que aceita os dois parâmetros\n",
    "def test_all_col_exists(df,cols):\n",
    "# Compreensão de lista que percorre cada elemento da lista\n",
    "    missing_cols = [col for col in cols if col not in df.columns]\n",
    "    \n",
    "    # O print irá trazer as colunas que não estão presentes no DF\n",
    "    if missing_cols:\n",
    "        print(f'A seguintes colunas não foram encontradas no {df}: {missing_cols}')\n",
    "    else:\n",
    "        print('Todas as colunas foram encontradas no DF')\n",
    "        \n",
    "# Colunas para verificação \n",
    "colunas_verificadas = [ 'TIMESTAMP', 'RECORD', 'Batt_volt_Min', 'PTemp', 'WindSpeed_1', 'WindDir_1', 'WindSpeed_2', 'WindDir_2',\n",
    "    'WindSpeed_3', 'WindDir_3', 'WindSpeed_4', 'WindDir_4', 'temp_1_Avg', 'temp_2_Avg', 'temp_3_Avg', 'temp_4_Avg',\n",
    "    'humid_1_Avg', 'humid_2_Avg', 'humid_3_Avg', 'humid_4_Avg', 'par1_in_Avg', 'par2_in_Avg', 'par3_in_Avg',\n",
    "    'par4_in_Avg', 'chuva_Tot', 'SBTempC_1_Avg', 'TargTempC_1_Avg', 'SBTempC_2_Avg', 'TargTempC_2_Avg',\n",
    "    'SBTempC_3_Avg', 'TargTempC_3_Avg', 'SBTempC_4_Avg', 'TargTempC_4_Avg', 'pressao_Avg', 'rad_total_Avg',\n",
    "    'rad_diff_Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d199bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "33 arquivos foram verificados\n"
     ]
    }
   ],
   "source": [
    "# Variável para armazenar parte do nome do arquivo\n",
    "meteo_1m = \"CR6_T1_meteo_\"\n",
    "arquivos_verificados = []\n",
    "for arquivo in arquivos_staging:\n",
    "    # Retornando apenas o nome do arquivo sem o caminho da pasta\n",
    "    nome_arquivo = os.path.basename(arquivo)\n",
    "    if meteo_1m in nome_arquivo:\n",
    "        df = pd.read_csv(arquivo, sep=',',skiprows = [0,2,3]) #header=1)\n",
    "        # Chama a função para verificar a presença das colunas\n",
    "        test_all_col_exists(df, colunas_verificadas)\n",
    "        arquivos_verificados.append(nome_arquivo)\n",
    "print(f'{len(arquivos_verificados)} arquivos foram verificados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67162ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for arquivos_verificados in arquivos_staging:\n",
    "    df = pd.read_csv(arquivo, sep=',',skiprows = [0,2,3]) #header=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25226064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista que armazena a quantidade de itens enviados\n",
    "qntd_enviada = []\n",
    "\n",
    "for arquivos_verificados in arquivos_staging:\n",
    "    # Buscar o caminho desse arquivo na staging\n",
    "    caminho_arquivo_staging = f'{pasta_Staging}\\\\{arquivos_verificados}'\n",
    "    # Buscar a pasta Staging no GDrive\n",
    "    caminho_arquivo_gdrive = f'{pasta_Gdrive_1m}\\\\{nome_arquivo}'\n",
    "    # Copiar o arquivo de uma pasta para outra\n",
    "    shutil.copy(caminho_arquivo_one, caminho_arquivo_gdrive)\n",
    "    # Para cada arquivo 1 mensagem\n",
    "    print(f'Arquivo {nome_arquivo} copiado para Gdrive')\n",
    "    # Adicionar +1 para a lista\n",
    "    qntd_enviada.append(+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60544117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chama a função para verificar a presença das colunas\n",
    "#test_all_col_exists(df, colunas_verificadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266a0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
