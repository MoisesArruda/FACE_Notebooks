{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5871d5c9",
   "metadata": {},
   "source": [
    "### Este notebook possui as seguintes finalidades\n",
    "\n",
    "1 - Consultar os arquivos da pasta Staging no Google Drive.\n",
    "2 - Pegar todos os arquivos e transformar em DF.\n",
    "3 - Retornar os cabeçalhos.\n",
    "4 - Gerar CSV final\n",
    "5 - Armazenar os arquivos .dat na pasta correta 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43df890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabalhar com DF\n",
    "import pandas as pd\n",
    "# Acessar sistema operacional\n",
    "import os\n",
    "# Irá retornar uma lista de arquivos\n",
    "import glob\n",
    "# Manipulação de arquivos e diretórios\n",
    "import shutil\n",
    "# Identificar o separador do arquivo\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9bee9d",
   "metadata": {},
   "source": [
    "###### Todos os caminhos das pastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1ba059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onde esses arquivos serão armazenados\n",
    "pasta_Staging = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging'\n",
    "# Arquivos de 1 minuto\n",
    "pasta_Gdrive_1m = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_Meteo_1m_2023\\Ascii'\n",
    "# Caminho csv 1m\n",
    "arquivo_csv = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_Meteo_1m_2023.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e76b8a",
   "metadata": {},
   "source": [
    "###### Arquivos de 1 Minuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9796bbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro: A pasta está vazia.\n"
     ]
    }
   ],
   "source": [
    "# Obter a lista de arquivos .dat na pasta 1 minuto\n",
    "arquivos_staging = glob.glob(f'{pasta_Staging}\\\\*.dat')\n",
    "\n",
    "# Se não houve nenhum arquivo\n",
    "if not arquivos_staging:\n",
    "    print('Erro: A pasta está vazia.')\n",
    "    exit() # Terminar o programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e40a8d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(arquivo_csv,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,error_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pd\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mParserErros \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(arquivo_csv,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,error_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pd\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mParserErros \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     linhas_com_erros \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(line\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Adicione os índices das linhas problemáticas à lista linhas_problematicas\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Há algumas linhas com erro no CSV, irei pula-las e relatar \n",
    "linhas_problematicas = []\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(arquivo_csv,sep=\",\",error_bad_lines=False)\n",
    "except pd.errors.ParserErros as e:\n",
    "    linhas_com_erros = [int(line.split()[0]) for line in str(e).split('\\n')[1:-1]]\n",
    "    \n",
    "    # Adicione os índices das linhas problemáticas à lista linhas_problematicas\n",
    "    linhas_problematicas.extend(linhas_com_erros)\n",
    "    \n",
    "print(\"Índices das linhas problemáticas:\", linhas_problematicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03488e",
   "metadata": {},
   "source": [
    "##### Teste para verificar se todas as colunas estão presentes nos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fad979cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da função que aceita os dois parâmetros\n",
    "def test_all_col_exists(df_1,cols):\n",
    "# Compreensão de lista que percorre cada elemento da lista\n",
    "    missing_cols = [col for col in cols if col not in df_1.columns]\n",
    "    \n",
    "    # O print irá trazer as colunas que não estão presentes no DF\n",
    "    if missing_cols:\n",
    "        print(f'A seguintes colunas não foram encontradas no {df_1}: {missing_cols}')\n",
    "    else:\n",
    "        print('Todas as colunas foram encontradas no DF')\n",
    "        \n",
    "# Colunas para verificação \n",
    "colunas_verificadas = [ 'TIMESTAMP', 'RECORD', 'Batt_volt_Min', 'PTemp', 'WindSpeed_1', 'WindDir_1', 'WindSpeed_2', 'WindDir_2',\n",
    "    'WindSpeed_3', 'WindDir_3', 'WindSpeed_4', 'WindDir_4', 'temp_1_Avg', 'temp_2_Avg', 'temp_3_Avg', 'temp_4_Avg',\n",
    "    'humid_1_Avg', 'humid_2_Avg', 'humid_3_Avg', 'humid_4_Avg', 'par1_in_Avg', 'par2_in_Avg', 'par3_in_Avg',\n",
    "    'par4_in_Avg', 'chuva_Tot', 'SBTempC_1_Avg', 'TargTempC_1_Avg', 'SBTempC_2_Avg', 'TargTempC_2_Avg',\n",
    "    'SBTempC_3_Avg', 'TargTempC_3_Avg', 'SBTempC_4_Avg', 'TargTempC_4_Avg', 'pressao_Avg', 'rad_total_Avg',\n",
    "    'rad_diff_Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d199bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "33 arquivos foram verificados\n"
     ]
    }
   ],
   "source": [
    "# Variável para armazenar parte do nome do arquivo\n",
    "meteo_1m = \"CR6_T1_meteo_\"\n",
    "# Lista dos arquivos verificados para ser utilizada posteriormente\n",
    "arquivos_verificados = []\n",
    "\n",
    "\n",
    "for arquivo in arquivos_staging:\n",
    "    # Retornando apenas o nome do arquivo sem o caminho da pasta\n",
    "    nome_arquivo = os.path.basename(arquivo)\n",
    "    \n",
    "    if meteo_1m in nome_arquivo:\n",
    "        \n",
    "        df_1 = pd.read_csv(arquivo, sep=',',skiprows = [0,2,3]) #header=1)\n",
    "        # Chama a função para verificar a presença das colunas\n",
    "        test_all_col_exists(df_1, colunas_verificadas)\n",
    "        arquivos_verificados.append(nome_arquivo)\n",
    "        \n",
    "print(f'{len(arquivos_verificados)} arquivos foram verificados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "538d2f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340843\n"
     ]
    }
   ],
   "source": [
    "# Consultar a quantidade de linhas do arquivo csv\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7cfa7",
   "metadata": {},
   "source": [
    "##### Função teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3522474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para detectar o separados dos arquivos\n",
    "def detectar_separador(arquivo):\n",
    "    with open(arquivo, 'r') as file:\n",
    "        conteudo = file.read(1024)  # Leitura dos primeiros 1024 bytes para detectar o separador\n",
    "        if ';' in conteudo:\n",
    "            return ';'\n",
    "        elif ',' in conteudo:\n",
    "            return ','\n",
    "        else:\n",
    "            return '\\t'  # Use '\\t' como separador padrão se nenhum for detectado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e616738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'separdor_virgula = []\\nseparador_ponto = []\\nseparador_tabula = []\\n\\nfro'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"separdor_virgula = []\n",
    "separador_ponto = []\n",
    "separador_tabula = []\n",
    "\n",
    "fro\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e59bc0",
   "metadata": {},
   "source": [
    "##### Gerar o arquivo csv sem duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67162ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soma_linhas = 0\n",
    "nome_csv = R'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\Teste.csv'\n",
    "\n",
    "# Crie um DataFrame vazio para iniciar\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "for arquivo in arquivos_staging:\n",
    "    #separador = detectar_separador(arquivos_verificados)\n",
    "    nome_arquivo = os.path.basename(arquivo)\n",
    "    if nome_arquivo in arquivos_verificados:\n",
    "    \n",
    "         # Ler o arquivo usando o separador detectado\n",
    "        df_1 = pd.read_csv(arquivo,sep=detectar_separador(arquivo), skiprows=[0, 2, 3])\n",
    "    \n",
    "    # Atualize o contador de linhas\n",
    "        soma_linhas += df_1.shape[0]\n",
    "    # Concatene o DataFrame lido ao df_final existente para cada iteração\n",
    "        df_final = pd.concat([df_final,df_1],ignore_index=True)\n",
    "\n",
    "# Gerando o df_final sem duplicatas\n",
    "df_sem_duplicatas = df_final.drop_duplicates()\n",
    "\n",
    "# Ordenando o DF pela coluna Record\n",
    "df_sem_duplicatas = df_sem_duplicatas.sort_values(by=\"RECORD\")\n",
    "# Resetar o indice\n",
    "df_sem_duplicatas = df_sem_duplicatas.reset_index(drop=True)\n",
    "\n",
    "df_sem_duplicatas.head()\n",
    "# Use o método to_csv para salvar o DataFrame em um arquivo CSV\n",
    "df_sem_duplicatas.to_csv(nome_csv,sep=\",\", index=False)\n",
    "#print(f'Arquivo {nome_arquivo} gerado com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41cc379",
   "metadata": {},
   "source": [
    "##### Realizar a leitura do arquivo CSV gerado, passo não necessário "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8416a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"nome_arquivo = \"Teste.csv\"\n",
    "\n",
    "with open(nome_arquivo, 'w', newline='') as csvfile:\n",
    "    # Especifique o delimitador de células (pode ser tabulação, ponto e vírgula, etc.)\n",
    "    delimitador = '\\t'  # Neste exemplo, usamos tabulação como delimitador\n",
    "    \n",
    "    # Crie um escritor CSV usando o delimitador especificado\n",
    "    csvwriter = csv.writer(csvfile, delimiter=delimitador)\n",
    "    \n",
    "    # Escreva os dados do DataFrame no arquivo CSV\n",
    "    for _, linha in df_sem_duplicatas.iterrows():\n",
    "        # Converte cada valor da linha em uma lista e escreve no arquivo\n",
    "        csvwriter.writerow(linha.tolist())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d6babd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51837\n",
      "51837\n"
     ]
    }
   ],
   "source": [
    "print(df_sem_duplicatas.shape[0])\n",
    "print(soma_linhas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c9549",
   "metadata": {},
   "source": [
    "##### Isso irá automatizar meu processo de incrementar o arquivo gerado, mas antes preciso gerar o arquivo csv final separado por virgulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c787c655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Concatenando ao longo das linhas Axis=0\\ndf_concat = pd.concat([df,df_sem_duplicatas],axis=0)\\ndf_to_csv = df_concat.sort_values(by=\"RECORD\") \\ndf_to_csv = df_to_csv.reset_index(drop=True)\\ndf_to_csv.to_csv(nome_csv, index=False)\\nprint(f\\'Arquivo {nome_csv} gerado com sucesso.\\')\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Concatenando ao longo das linhas Axis=0\n",
    "df_concat = pd.concat([df,df_sem_duplicatas],axis=0)\n",
    "df_to_csv = df_concat.sort_values(by=\"RECORD\") \n",
    "df_to_csv = df_to_csv.reset_index(drop=True)\n",
    "df_to_csv.to_csv(nome_csv, index=False)\n",
    "print(f'Arquivo {nome_csv} gerado com sucesso.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63d36fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392680\n"
     ]
    }
   ],
   "source": [
    "print(df_to_csv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b0f9f",
   "metadata": {},
   "source": [
    "##### Enviar os arquivos verificados para a pasta T1_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25226064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_08_31_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_08_31_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_01_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_01_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_02_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_02_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_03_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_03_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_04_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_04_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_05_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_05_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_06_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_06_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_07_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_07_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_08_0001.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_08_0001.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_09_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_09_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_10_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_10_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_11_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_11_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_12_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_12_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_13_0855.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_13_0855.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_14_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_14_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_15_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_15_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_16_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_16_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_20_1430.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_20_1430.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_21_0001.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_21_0001.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_22_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_22_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_23_0001.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_23_0001.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_24_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_24_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_25_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_25_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_26_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_26_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_27_0031.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_27_0031.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_27_0038.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_27_0038.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_27_0148.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_27_0148.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_28_0111.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_28_0111.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_09_29_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_09_29_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_10_02_0747.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_10_02_0747.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_10_03_1801.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_10_03_1801.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_10_04_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_10_04_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteo_2023_10_05_0000.dat copiado para Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteo_2023_10_05_0000.dat excluído com sucesso\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Lista que armazena a quantidade de itens enviados\n",
    "qntd_enviada = []\n",
    "qntd_excluida = []\n",
    "\n",
    "for arquivo in arquivos_staging:\n",
    "    nome_arquivo = os.path.basename(arquivo)\n",
    "    if nome_arquivo in arquivos_verificados:\n",
    "        # Buscar o caminho desse arquivo na staging\n",
    "        caminho_arquivo_staging = f'{pasta_Staging}\\\\{nome_arquivo}'\n",
    "        # Buscar a pasta Staging no GDrive\n",
    "        caminho_arquivo_gdrive = f'{pasta_Gdrive_1m}'\n",
    "        # Copiar o arquivo de uma pasta para outra\n",
    "        shutil.copy(caminho_arquivo_staging, caminho_arquivo_gdrive)\n",
    "        # Para cada arquivo 1 mensagem\n",
    "        print(f'Arquivo {nome_arquivo} copiado para Gdrive')\n",
    "        # Adicionar +1 para a lista\n",
    "        qntd_enviada.append(+1)\n",
    "        # Código abaixo é para deletar os arquivos que foram transferidos para a outra pasta\n",
    "        if os.path.exists(caminho_arquivo_staging):\n",
    "            #os.remove é usado para excluir só o arquivo, não as pasta\n",
    "            os.remove(caminho_arquivo_staging)\n",
    "            print(f'Arquivo {caminho_arquivo_staging} excluído com sucesso')\n",
    "            qntd_excluida.append(+1)\n",
    "        else:\n",
    "            print(f'O arquivo {caminho_arquivo_staging} não existe')\n",
    "print(qntd_enviada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60544117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# Chama a função para verificar a presença das colunas\n",
    "#test_all_col_exists(df, colunas_verificadas)\n",
    "print(len(qntd_enviada))\n",
    "print(len(qntd_excluida))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081de41",
   "metadata": {},
   "source": [
    "##### Próximo passo, fazer a leitura dos arquivos de 1m novamente e gerar o arquivo CSV separado por virgulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca173ca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ab3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
