{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99f5f10",
   "metadata": {},
   "source": [
    "### Este notebook possui as seguintes finalidades\n",
    "\n",
    "1- Consultar os arquivos da pasta Staging no Google Drive.\n",
    "2 - Pegar todos os arquivos e transformar em um DF.\n",
    "3 - Retornar os cabeçalhos.\n",
    "4 - Gerar CSV final.\n",
    "5 - Armazenar os arquivos .dat na pasta 30m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f86f22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabalhar com DF\n",
    "import pandas as pd\n",
    "# Acessar sistema operacional\n",
    "import os\n",
    "# Irá retornar uma lista de arquivos\n",
    "import glob\n",
    "# Manipulação de arquivos e diretórios\n",
    "import shutil\n",
    "# Identificar o separador do arquivo\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e493f25",
   "metadata": {},
   "source": [
    "##### Todos os caminhos das pastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0f384b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onde esses arquivos serão armazenados\n",
    "pasta_Staging = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging'\n",
    "# Arquivos de 30 minutos\n",
    "pasta_Gdrive_30m = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_meteo_Media_30m_2023\\Ascii'\n",
    "# Caminho csv 1m\n",
    "arquivo_csv = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_MeteoMedia_30m_2023.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddb0ad",
   "metadata": {},
   "source": [
    "##### Arquivos de 30 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d595889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter a lista de arquivos .dat na pasta 1 minuto\n",
    "\n",
    "arquivos_staging = glob.glob(f'{pasta_Staging}\\\\*.dat')\n",
    "\n",
    "# Se não houver nenhum arquivo\n",
    "if not arquivos_staging:\n",
    "    print('Erro: a pasta está vazia.')\n",
    "    # Terminar o programa.\n",
    "    exit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e789a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índices das linhas problemáticas: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BlueShift\\AppData\\Local\\Temp\\ipykernel_55088\\1069502122.py:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(arquivo_csv,sep=\",\",error_bad_lines=False)\n",
      "Skipping line 613: expected 1 fields, saw 2\n",
      "Skipping line 664: expected 1 fields, saw 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Há algumas linhas com erro no CSV, irei pula-las e relatar \n",
    "linhas_problematicas = []\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(arquivo_csv,sep=\",\",error_bad_lines=False)\n",
    "except pd.errors.ParserErros as e:\n",
    "    linhas_com_erros = [int(line.split()[0]) for line in str(e).split('\\n')[1:-1]]\n",
    "    \n",
    "    # Adicione os índices das linhas problemáticas à lista linhas_problematicas\n",
    "    linhas_problematicas.extend(linhas_com_erros)\n",
    "    \n",
    "print(\"Índices das linhas problemáticas:\", linhas_problematicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac000f1",
   "metadata": {},
   "source": [
    "##### Teste para verificar se todas as colunas estão presentes nos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f26c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da função que aceita os dois parâmetros\n",
    "def test_all_col_exists(df_1,cols):\n",
    "    # Compreensão de lista que percorre cada elemento da lista\n",
    "    missing_cols = [col for col in cols if col not in df_1.columns]\n",
    "    \n",
    "    # O print irá trazer as colunas que não estão presentes no DF\n",
    "    if missing_cols:\n",
    "        print(f'A seguintes colunas não foram encontradas no {df_1}: {missing_cols}')\n",
    "    else:\n",
    "        print('Todas as colunas foram encontradas no DF')\n",
    "        \n",
    "# Colunas para verificação \n",
    "colunas_verificadas = [ 'TIMESTAMP', 'RECORD', 'WindSpeed_1', 'WindDir_1', 'WindSpeed_2', 'WindDir_2',\n",
    "    'WindSpeed_3', 'WindDir_3', 'WindSpeed_4', 'WindDir_4', 'temp_1_Avg', 'temp_2_Avg', 'temp_3_Avg', 'temp_4_Avg',\n",
    "    'humid_1_Avg', 'humid_2_Avg', 'humid_3_Avg', 'humid_4_Avg', 'par1_in_Avg', 'par2_in_Avg', 'par3_in_Avg',\n",
    "    'par4_in_Avg', 'chuva_Tot', 'SBTempC_1_Avg', 'TargTempC_1_Avg', 'SBTempC_2_Avg', 'TargTempC_2_Avg',\n",
    "    'SBTempC_3_Avg', 'TargTempC_3_Avg', 'SBTempC_4_Avg', 'TargTempC_4_Avg', 'pressao_Avg', 'rad_total_Avg',\n",
    "    'rad_diff_Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "014a587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "Todas as colunas foram encontradas no DF\n",
      "32 arquivos foram verificados\n"
     ]
    }
   ],
   "source": [
    "# Variável para armazenar parte do nome do arquivo\n",
    "meteo_30m = \"meteoMedia30\"\n",
    "# Lista dos arquivos verificados para ser utilizada posteriormente\n",
    "arquivos_verificados = []\n",
    "\n",
    "for arquivo in arquivos_staging:\n",
    "    # Retornando apenas o nome do arquivo sem o caminho da pasta\n",
    "    nome_arquivo = os.path.basename(arquivo)\n",
    "    \n",
    "    if meteo_30m in nome_arquivo:\n",
    "        df_1 = pd.read_csv(arquivo, sep=\",\",skiprows = [0,2,3]) #header=1)\n",
    "        # Chama a função para verificar a presença das colunas\n",
    "        test_all_col_exists(df_1,colunas_verificadas)\n",
    "        arquivos_verificados.append(nome_arquivo)\n",
    "print(f'{len(arquivos_verificados)} arquivos foram verificados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "83c299c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11304\n"
     ]
    }
   ],
   "source": [
    "# Consultar a quantidade de linhas do arquivo csv\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "889a3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_separador(arquivo):\n",
    "    with open(arquivo,'r') as file:\n",
    "        # Leitura dos primeiros 1024 bytes para detectar o separador\n",
    "        conteudo = file.read(1024)\n",
    "        if ';' in conteudo:\n",
    "            return ';'\n",
    "        elif ',' in conteudo:\n",
    "            return ','\n",
    "        else:\n",
    "            # Use '\\t' como separador padrão se nenhum for detectado\n",
    "            return '\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff3735",
   "metadata": {},
   "source": [
    "##### Gerar o arquivo csv sem duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "662d5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_linhas = 0\n",
    "nome_csv = R'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\Teste_30m.csv'\n",
    "\n",
    "# Crie um DataFrame vazio para iniciar\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "for arquivo in arquivos_staging:\n",
    "    #separador = detectar_separador(arquivos_verificados)\n",
    "    nome_arquivo = os.path.basename(arquivo)\n",
    "    if nome_arquivo in arquivos_verificados:\n",
    "        \n",
    "        # Ler o arquivo usando o separador detectado\n",
    "        df_1 = pd.read_csv(arquivo,sep=detectar_separador(arquivo), skiprows=[0, 2, 3])\n",
    "        # Atualize o contador de linhas\n",
    "        soma_linhas +=df_1.shape[0]\n",
    "        # Concatene o DataFrame lido ao df_final existente para cada iteração\n",
    "        df_final = pd.concat([df_final,df_1],ignore_index = True)\n",
    "\n",
    "# Gerando o df_final sem duplicatas\n",
    "df_sem_duplicatas = df_final.drop_duplicates()\n",
    "\n",
    "# Ordenando o DF pela coluna Record\n",
    "df_sem_duplicatas = df_sem_duplicatas.sort_values(by='RECORD')\n",
    "# Resetar o indice\n",
    "df_sem_duplicatas = df_sem_duplicatas.reset_index(drop=True)\n",
    "\n",
    "df_sem_duplicatas.head()\n",
    "# Use o método to_csv para salvar o DataFrame em um arquivo CSV\n",
    "df_sem_duplicatas.to_csv(nome_csv,sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0353f5",
   "metadata": {},
   "source": [
    "##### Realizar a leitura do arquivo CSV gerado, passo não necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90c526dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nome_arquivo = \"Teste.csv\"\\n\\nwith open(nome_arquivo, \\'w\\', newline=\\'\\') as csvfile:\\n    # Especifique o delimitador de células (pode ser tabulação, ponto e vírgula, etc.)\\n    delimitador = \\'\\t\\'  # Neste exemplo, usamos tabulação como delimitador\\n    \\n    # Crie um escritor CSV usando o delimitador especificado\\n    csvwriter = csv.writer(csvfile, delimiter=delimitador)\\n    \\n    # Escreva os dados do DataFrame no arquivo CSV\\n    for _, linha in df_sem_duplicatas.iterrows():\\n        # Converte cada valor da linha em uma lista e escreve no arquivo\\n        csvwriter.writerow(linha.tolist())\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"nome_arquivo = \"Teste.csv\"\n",
    "\n",
    "with open(nome_arquivo, 'w', newline='') as csvfile:\n",
    "    # Especifique o delimitador de células (pode ser tabulação, ponto e vírgula, etc.)\n",
    "    delimitador = '\\t'  # Neste exemplo, usamos tabulação como delimitador\n",
    "    \n",
    "    # Crie um escritor CSV usando o delimitador especificado\n",
    "    csvwriter = csv.writer(csvfile, delimiter=delimitador)\n",
    "    \n",
    "    # Escreva os dados do DataFrame no arquivo CSV\n",
    "    for _, linha in df_sem_duplicatas.iterrows():\n",
    "        # Converte cada valor da linha em uma lista e escreve no arquivo\n",
    "        csvwriter.writerow(linha.tolist())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5998e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725\n",
      "1725\n"
     ]
    }
   ],
   "source": [
    "print(df_sem_duplicatas.shape[0])\n",
    "print(soma_linhas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888894c",
   "metadata": {},
   "source": [
    "##### Isso irá automatizar meu processo de incrementar o arquivo gerado, mas antes preciso gerar o arquivo csv final separado por virgulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e40e33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Concatenando ao longo das linhas Axis=0\\ndf_concat = pd.concat([df,df_sem_duplicatas],axis=0)\\ndf_to_csv = df_concat.sort_values(by=\"RECORD\") \\ndf_to_csv = df_to_csv.reset_index(drop=True)\\ndf_to_csv.to_csv(nome_csv, index=False)\\nprint(f\\'Arquivo {nome_csv} gerado com sucesso.\\')\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Concatenando ao longo das linhas Axis=0\n",
    "df_concat = pd.concat([df,df_sem_duplicatas],axis=0)\n",
    "df_to_csv = df_concat.sort_values(by=\"RECORD\") \n",
    "df_to_csv = df_to_csv.reset_index(drop=True)\n",
    "df_to_csv.to_csv(nome_csv, index=False)\n",
    "print(f'Arquivo {nome_csv} gerado com sucesso.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c49c6e3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_to_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_to_csv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_to_csv' is not defined"
     ]
    }
   ],
   "source": [
    "print(df_to_csv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77cdf7",
   "metadata": {},
   "source": [
    "#### Enviar os arquivos verificados para a pasta T1_30m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f6e4dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_08_31_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_08_31_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_01_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_01_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_02_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_02_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_03_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_03_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_04_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_04_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_05_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_05_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_06_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_06_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_07_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_07_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_08_0030.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_08_0030.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_09_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_09_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_10_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_10_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_11_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_11_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_12_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_12_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_13_0900.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_13_0900.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_14_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_14_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_15_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_15_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_16_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_16_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_20_1430.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_20_1430.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_21_0030.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_21_0030.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_22_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_22_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_23_0030.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_23_0030.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_24_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_24_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_25_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_25_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_26_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_26_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_27_0100.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_27_0100.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_27_0430.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_27_0430.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_28_0130.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_28_0130.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_09_29_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_09_29_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_10_02_0800.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_10_02_0800.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_10_03_1830.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_10_03_1830.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_10_04_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_10_04_0000.dat excluído com sucesso\n",
      "Arquivo AmazonFACE_CR6_T1_meteoMedia30_2023_10_05_0000.dat copiado para o Gdrive\n",
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\AmazonFACE_CR6_T1_meteoMedia30_2023_10_05_0000.dat excluído com sucesso\n",
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Lista que armazena a quantidade de itens enviados\n",
    "qntd_enviada = []\n",
    "qntd_excluida = []\n",
    "\n",
    "for arquivo in arquivos_staging:\n",
    "    nome_arquivo = os.path.basename(arquivo)\n",
    "    # Buscar o caminho desse arquivo na staging\n",
    "    caminho_arquivo_staging = f'{pasta_Staging}\\\\{nome_arquivo}'\n",
    "    # Buscar a pasta Staging no GDrive\n",
    "    caminho_arquivo_gdrive = f'{pasta_Gdrive_30m}'\n",
    "    # Copiar o arquivo de uma pasta para outra\n",
    "    shutil.copy(caminho_arquivo_staging, caminho_arquivo_gdrive)\n",
    "    # Para cada arquivo 1 mensagem\n",
    "    print(f'Arquivo {nome_arquivo} copiado para o Gdrive')\n",
    "    # Adicionar +1 para a lista\n",
    "    qntd_enviada.append(+1)\n",
    "    # Código abaixo é para deletar os arquivos que foram transferidos para a outra pasta\n",
    "    if os.path.exists(caminho_arquivo_staging):\n",
    "        #os.remove é usado para excluir só o arquivo, não as pasta\n",
    "        os.remove(caminho_arquivo_staging)\n",
    "        print(f'Arquivo {caminho_arquivo_staging} excluído com sucesso')\n",
    "        qntd_excluida.append(+1)\n",
    "    else:\n",
    "        print(f'O arquivo{caminho_arquivo_staging} não existe')\n",
    "print(len(qntd_enviada))\n",
    "print(len(qntd_excluida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd34ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Neste passo to fazendo o teste para gerar os csv juntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4119dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BlueShift\\AppData\\Local\\Temp\\ipykernel_55088\\1113510778.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(nome_csv1,sep=';',error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "nome_csv1 = r'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\T1_MeteoMedia_30m_2023.csv'\n",
    "df = pd.read_csv(nome_csv1,sep=';',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1679ad64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>RECORD</th>\n",
       "      <th>WindSpeed_1</th>\n",
       "      <th>WindDir_1</th>\n",
       "      <th>WindSpeed_2</th>\n",
       "      <th>WindDir_2</th>\n",
       "      <th>WindSpeed_3</th>\n",
       "      <th>WindDir_3</th>\n",
       "      <th>WindSpeed_4</th>\n",
       "      <th>WindDir_4</th>\n",
       "      <th>...</th>\n",
       "      <th>TargTempC_1_Avg</th>\n",
       "      <th>SBTempC_2_Avg</th>\n",
       "      <th>TargTempC_2_Avg</th>\n",
       "      <th>SBTempC_3_Avg</th>\n",
       "      <th>TargTempC_3_Avg</th>\n",
       "      <th>SBTempC_4_Avg</th>\n",
       "      <th>TargTempC_4_Avg</th>\n",
       "      <th>pressao_Avg</th>\n",
       "      <th>rad_total_Avg</th>\n",
       "      <th>rad_diff_Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2023 00:00</td>\n",
       "      <td>4473</td>\n",
       "      <td>1.252.222</td>\n",
       "      <td>339.979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.236.845</td>\n",
       "      <td>2.236.646</td>\n",
       "      <td>1.834.034</td>\n",
       "      <td>2.240.627</td>\n",
       "      <td>2.006.042</td>\n",
       "      <td>2.233.097</td>\n",
       "      <td>1.977.277</td>\n",
       "      <td>9.962.813</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2023 00:30</td>\n",
       "      <td>4474</td>\n",
       "      <td>0.8597224</td>\n",
       "      <td>7.502.235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223.256</td>\n",
       "      <td>2.225.941</td>\n",
       "      <td>1.821.121</td>\n",
       "      <td>2.227.962</td>\n",
       "      <td>1.994.649</td>\n",
       "      <td>2.221.755</td>\n",
       "      <td>19.657</td>\n",
       "      <td>9.957.971</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2023 01:00</td>\n",
       "      <td>4475</td>\n",
       "      <td>1.006.111</td>\n",
       "      <td>8.029.311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223.549</td>\n",
       "      <td>2.219.495</td>\n",
       "      <td>1.820.764</td>\n",
       "      <td>2.220.754</td>\n",
       "      <td>1.991.658</td>\n",
       "      <td>2.215.539</td>\n",
       "      <td>1.964.685</td>\n",
       "      <td>9.951.842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2023 01:30</td>\n",
       "      <td>4476</td>\n",
       "      <td>1.181.666</td>\n",
       "      <td>7.483.522</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.208.759</td>\n",
       "      <td>2.209.636</td>\n",
       "      <td>1.808.706</td>\n",
       "      <td>2.211.353</td>\n",
       "      <td>1.980.769</td>\n",
       "      <td>2.206.184</td>\n",
       "      <td>1.952.285</td>\n",
       "      <td>9.945.773</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2023 02:00</td>\n",
       "      <td>4477</td>\n",
       "      <td>0.9066666</td>\n",
       "      <td>8.430.209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.203.237</td>\n",
       "      <td>2.200.943</td>\n",
       "      <td>1.802.293</td>\n",
       "      <td>2.202.038</td>\n",
       "      <td>197.412</td>\n",
       "      <td>2.196.756</td>\n",
       "      <td>194.692</td>\n",
       "      <td>9.942.584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          TIMESTAMP  RECORD WindSpeed_1  WindDir_1  WindSpeed_2  WindDir_2  \\\n",
       "0  01/01/2023 00:00    4473   1.252.222    339.979            0          0   \n",
       "1  01/01/2023 00:30    4474   0.8597224  7.502.235            0          0   \n",
       "2  01/01/2023 01:00    4475   1.006.111  8.029.311            0          0   \n",
       "3  01/01/2023 01:30    4476   1.181.666  7.483.522            0          0   \n",
       "4  01/01/2023 02:00    4477   0.9066666  8.430.209            0          0   \n",
       "\n",
       "   WindSpeed_3  WindDir_3  WindSpeed_4  WindDir_4  ... TargTempC_1_Avg  \\\n",
       "0            0          0            0          0  ...       2.236.845   \n",
       "1            0          0            0          0  ...       2.223.256   \n",
       "2            0          0            0          0  ...       2.223.549   \n",
       "3            0          0            0          0  ...       2.208.759   \n",
       "4            0          0            0          0  ...       2.203.237   \n",
       "\n",
       "  SBTempC_2_Avg TargTempC_2_Avg SBTempC_3_Avg TargTempC_3_Avg SBTempC_4_Avg  \\\n",
       "0     2.236.646       1.834.034     2.240.627       2.006.042     2.233.097   \n",
       "1     2.225.941       1.821.121     2.227.962       1.994.649     2.221.755   \n",
       "2     2.219.495       1.820.764     2.220.754       1.991.658     2.215.539   \n",
       "3     2.209.636       1.808.706     2.211.353       1.980.769     2.206.184   \n",
       "4     2.200.943       1.802.293     2.202.038         197.412     2.196.756   \n",
       "\n",
       "  TargTempC_4_Avg pressao_Avg rad_total_Avg rad_diff_Avg  \n",
       "0       1.977.277   9.962.813             0            0  \n",
       "1          19.657   9.957.971             0            0  \n",
       "2       1.964.685   9.951.842             0            0  \n",
       "3       1.952.285   9.945.773             0            0  \n",
       "4         194.692   9.942.584             0            0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "44a08307",
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_csv2 = R'I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\Teste_30m1.csv'\n",
    "df.to_csv(nome_csv2,sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1f7efb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo I:\\Meu Drive\\AmazonFace\\Dados das Torres do Face\\T1\\07 - 2023\\Staging\\Teste_30m.csv gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Concatenando ao longo das linhas Axis=0\n",
    "df_concat = pd.concat([df,df_sem_duplicatas],axis=0)\n",
    "df_to_csv = df_concat.sort_values(by=\"RECORD\") \n",
    "df_to_csv = df_to_csv.reset_index(drop=True)\n",
    "df_to_csv.to_csv(nome_csv2, index=False)\n",
    "print(f'Arquivo {nome_csv} gerado com sucesso.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
